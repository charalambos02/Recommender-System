import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from surprise import Dataset, Reader, SVD
from surprise.model_selection import cross_validate, train_test_split
from surprise.accuracy import rmse
import warnings
from typing import Dict, List, Union, Optional
import logging
from pathlib import Path
warnings.filterwarnings('ignore')
class HybridRecommender:
    """Hybrid recommendation system combining collaborative and content-based filtering.
    Attributes:
        alpha (float): Weight for collaborative filtering (0-1)
        interactions (pd.DataFrame): User-item interaction data
        item_data (pd.DataFrame): Item metadata including descriptions
        cf_model (SVD): Collaborative filtering model
        tfidf (TfidfVectorizer): Text vectorizer for content-based filtering
        cosine_sim (np.ndarray): Item similarity matrix
    """
    def __init__(self, interactions_path: str = 'user_item_interactions.csv',
                 metadata_path: str = 'item_metadata.csv', alpha: float = 0.5):
        """Initialize the recommender system with data paths and hybrid weight.
        Args:
            interactions_path: Path to CSV with user-item interactions
            metadata_path: Path to CSV with item metadata
            alpha: Weight for collaborative filtering (0-1); content-based weight = 1-alpha
        Raises:
            ValueError: If alpha is not between 0-1 or required columns are missing
            FileNotFoundError: If input files don't exist
        """
        if not 0 <= alpha <= 1:
            raise ValueError("Alpha must be between 0 and 1")
        self.alpha = alpha
        self._setup_logging()
        self.logger.info("Initializing HybridRecommender")
        try:
            self._load_data(interactions_path, metadata_path)
            self._init_collaborative_filtering()
            self._init_content_based_filtering()
            self._calculate_similarity_matrix()
        except Exception as e:
            self.logger.error(f"Initialization failed: {str(e)}")
            raise
    def _setup_logging(self):
        """Configure logging for the recommender system."""
        self.logger = logging.getLogger(__name__)
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    def _load_data(self, interactions_path: str, metadata_path: str):
        """Load and validate input data files.
        Args:
            interactions_path: Path to interactions CSV
            metadata_path: Path to metadata CSV
        Raises:
            ValueError: If required columns are missing
            FileNotFoundError: If files don't exist
        """
        if not Path(interactions_path).exists():
            raise FileNotFoundError(f"Interactions file not found: {interactions_path}")
        if not Path(metadata_path).exists():
            raise FileNotFoundError(f"Metadata file not found: {metadata_path}")
        self.interactions = pd.read_csv(interactions_path)
        self.item_data = pd.read_csv(metadata_path)
        required_interaction_cols = ['user_id', 'item_id', 'rating']
        required_metadata_cols = ['item_id', 'description']
        if not all(col in self.interactions.columns for col in required_interaction_cols):
            raise ValueError(f"Interactions data missing required columns: {required_interaction_cols}")
        if not all(col in self.item_data.columns for col in required_metadata_cols):
            raise ValueError(f"Item metadata missing required columns: {required_metadata_cols}")
        self.item_data['description'] = self.item_data['description'].fillna('')
        self.logger.info("Data loaded successfully")
    def _init_collaborative_filtering(self):
        """Initialize and train collaborative filtering model (SVD)."""
        self.logger.info("Initializing collaborative filtering model")
        reader = Reader(rating_scale=(1, 5))
        data = Dataset.load_from_df(self.interactions[['user_id', 'item_id', 'rating']], reader)
        trainset, testset = train_test_split(data, test_size=0.25)
        self.cf_model = SVD()
        self.cf_model.fit(trainset)
        predictions = self.cf_model.test(testset)
        self.logger.info(f"CF model trained - RMSE: {rmse(predictions):.4f}")
    def _init_content_based_filtering(self):
        """Initialize content-based filtering components."""
        self.logger.info("Initializing content-based filtering")
        self.tfidf = TfidfVectorizer(stop_words='english')
        self.tfidf_matrix = self.tfidf.fit_transform(self.item_data['description'])
    def _calculate_similarity_matrix(self):
        """Calculate cosine similarity matrix for items."""
        self.logger.info("Calculating item similarity matrix")
        self.cosine_sim = linear_kernel(self.tfidf_matrix, self.tfidf_matrix)
    def recommend_items(self, user_id: int, item_id: int = None, n: int = 10) -> pd.DataFrame:
        """Generate hybrid recommendations for a user.
        Args:
            user_id: Target user ID
            item_id: Optional seed item ID for content-based recommendations
            n: Number of recommendations to return
        Returns:
            DataFrame with recommended items and scores
        """
        # Collaborative filtering predictions
        all_items = self.item_data['item_id'].unique()
        cf_predictions = []
        for iid in all_items:
            pred = self.cf_model.predict(user_id, iid)
            cf_predictions.append((iid, pred.est))
        cf_recs = pd.DataFrame(cf_predictions, columns=['item_id', 'cf_score'])
        # Content-based recommendations
        if item_id is not None:
            idx = self.item_data.index[self.item_data['item_id'] == item_id].tolist()[0]
            sim_scores = list(enumerate(self.cosine_sim[idx]))
            sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
            sim_scores = sim_scores[1:n+1]
            item_indices = [i[0] for i in sim_scores]
            cb_recs = self.item_data.iloc[item_indices][['item_id']]
            cb_recs['cb_score'] = [i[1] for i in sim_scores]
        else:
            cb_recs = pd.DataFrame(columns=['item_id', 'cb_score'])
        # Combine recommendations
        if not cb_recs.empty:
            recommendations = pd.merge(cf_recs, cb_recs, on='item_id', how='outer')
            recommendations['hybrid_score'] = (self.alpha * recommendations['cf_score'].fillna(0) + 
                                            (1 - self.alpha) * recommendations['cb_score'].fillna(0))
        else:
            recommendations = cf_recs
            recommendations['hybrid_score'] = recommendations['cf_score']
        recommendations = recommendations.sort_values('hybrid_score', ascending=False).head(n)
        return recommendations.merge(self.item_data, on='item_id')
    def save_model(self, path: str):
        """Save the trained model components to disk."""
        import joblib
        model_data = {
            'alpha': self.alpha,
            'tfidf': self.tfidf,
            'cf_model': self.cf_model,
            'cosine_sim': self.cosine_sim,
            'item_data': self.item_data
        }
        joblib.dump(model_data, path)
        self.logger.info(f"Model saved to {path}")
    @classmethod
    def load_model(cls, path: str):
        """Load a saved model from disk."""
        import joblib
        model_data = joblib.load(path)
        recommender = cls.__new__(cls)
        recommender.alpha = model_data['alpha']
        recommender.tfidf = model_data['tfidf']
        recommender.cf_model = model_data['cf_model']
        recommender.cosine_sim = model_data['cosine_sim']
        recommender.item_data = model_data['item_data']
        return recommender