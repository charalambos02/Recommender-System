import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from surprise import Dataset, Reader, SVD
from surprise.model_selection import cross_validate, train_test_split
from surprise.accuracy import rmse
import warnings
from typing import Dict, List, Union, Optional, Tuple
import logging
from pathlib import Path
import pickle
warnings.filterwarnings('ignore')
class HybridRecommender:
    """Hybrid recommendation system combining collaborative and content-based filtering.
    The system combines matrix factorization (SVD) for collaborative filtering with
    TF-IDF vectorization for content-based recommendations, using a weighted average
    of both approaches.
    Attributes:
        alpha (float): Weight for collaborative filtering (0-1)
        interactions (pd.DataFrame): User-item interaction data with columns ['user_id', 'item_id', 'rating']
        item_data (pd.DataFrame): Item metadata with columns ['item_id', 'description', ...]
        cf_model (SVD): Trained collaborative filtering model
        tfidf (TfidfVectorizer): Fitted TF-IDF vectorizer
        cosine_sim (np.ndarray): Precomputed cosine similarity matrix
        logger (logging.Logger): Configured logger instance
    """
    REQUIRED_INTERACTION_COLS = {'user_id', 'item_id', 'rating'}
    REQUIRED_ITEM_COLS = {'item_id', 'description'}
    def __init__(self, interactions_path: str = 'user_item_interactions.csv',
                 metadata_path: str = 'item_metadata.csv', alpha: float = 0.5):
        """Initialize the recommender system with data paths and hybrid weight.
        Args:
            interactions_path: Path to CSV with user-item interactions
            metadata_path: Path to CSV with item metadata
            alpha: Weight for collaborative filtering (0-1); content-based weight = 1-alpha
        Raises:
            ValueError: If alpha is not between 0-1 or required columns are missing
            FileNotFoundError: If input files don't exist
            KeyError: If required columns are missing from input files
        """
        if not 0 <= alpha <= 1:
            raise ValueError("Alpha must be between 0 and 1")
        self.alpha = alpha
        self._setup_logging()
        self.logger.info(f"Initializing HybridRecommender with alpha={alpha}")
        self._load_data(interactions_path, metadata_path)
        self._validate_data()
        self._train_models()
    def _setup_logging(self) -> None:
        """Configure logging for the recommender system."""
        self.logger = logging.getLogger('HybridRecommender')
        self.logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    def _load_data(self, interactions_path: str, metadata_path: str) -> None:
        """Load interaction and metadata files.
        Args:
            interactions_path: Path to user-item interactions CSV
            metadata_path: Path to item metadata CSV
        Raises:
            FileNotFoundError: If either file cannot be found
        """
        try:
            self.interactions = pd.read_csv(interactions_path)
            self.item_data = pd.read_csv(metadata_path)
            self.logger.info(f"Successfully loaded data from {interactions_path} and {metadata_path}")
        except FileNotFoundError as e:
            self.logger.error(f"File not found: {e.filename}")
            raise
    def _validate_data(self) -> None:
        """Validate that loaded data contains required columns.
        Raises:
            KeyError: If required columns are missing
            ValueError: If data contains invalid values
        """
        # Check for required columns
        if not self.REQUIRED_INTERACTION_COLS.issubset(self.interactions.columns):
            missing = self.REQUIRED_INTERACTION_COLS - set(self.interactions.columns)
            raise KeyError(f"Missing columns in interactions data: {missing}")
        if not self.REQUIRED_ITEM_COLS.issubset(self.item_data.columns):
            missing = self.REQUIRED_ITEM_COLS - set(self.item_data.columns)
            raise KeyError(f"Missing columns in item data: {missing}")
        # Check for NaN values
        if self.interactions.isnull().values.any():
            raise ValueError("Interaction data contains null values")
        if self.item_data.isnull().values.any():
            raise ValueError("Item data contains null values")
        self.logger.info("Data validation passed")
    def _train_models(self) -> None:
        """Train both collaborative and content-based models."""
        self._train_collaborative_filtering()
        self._train_content_based()
        self.logger.info("Completed model training")
    def _train_collaborative_filtering(self) -> None:
        """Train SVD model for collaborative filtering."""
        reader = Reader(rating_scale=(1, 5))
        data = Dataset.load_from_df(self.interactions[['user_id', 'item_id', 'rating']], reader)
        trainset = data.build_full_trainset()
        self.cf_model = SVD()
        self.cf_model.fit(trainset)
        self.logger.info("Collaborative filtering model trained")
    def _train_content_based(self) -> None:
        """Train TF-IDF vectorizer and compute similarity matrix."""
        self.tfidf = TfidfVectorizer(stop_words='english')
        tfidf_matrix = self.tfidf.fit_transform(self.item_data['description'].fillna(''))
        self.cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
        self.logger.info("Content-based model trained")
    def recommend_items(self, user_id: int, n: int = 10) -> pd.DataFrame:
        """Generate hybrid recommendations for a user.
        Args:
            user_id: ID of user to generate recommendations for
            n: Number of recommendations to return
        Returns:
            DataFrame with columns ['item_id', 'hybrid_score', 'cf_score', 'cb_score']
            sorted by hybrid_score in descending order
        Raises:
            ValueError: If user_id is not found in interactions
        """
        if user_id not in self.interactions['user_id'].values:
            raise ValueError(f"User {user_id} not found in interactions")
        # Get items the user hasn't interacted with
        interacted_items = self.interactions[self.interactions['user_id'] == user_id]['item_id']
        candidate_items = self.item_data[~self.item_data['item_id'].isin(interacted_items)]
        # Generate collaborative filtering scores
        cf_scores = []
        for item_id in candidate_items['item_id']:
            cf_scores.append(self.cf_model.predict(user_id, item_id).est)
        # Generate content-based scores (using average similarity to user's liked items)
        user_items = self.interactions[self.interactions['user_id'] == user_id]
        top_user_items = user_items.sort_values('rating', ascending=False)['item_id'].head(5)
        cb_scores = []
        for item_idx, row in candidate_items.iterrows():
            sim_scores = []
            for liked_item in top_user_items:
                liked_idx = self.item_data[self.item_data['item_id'] == liked_item].index[0]
                current_idx = self.item_data[self.item_data['item_id'] == row['item_id']].index[0]
                sim_scores.append(self.cosine_sim[liked_idx][current_idx])
            cb_scores.append(np.mean(sim_scores))
        # Combine scores
        cf_scores = np.array(cf_scores)
        cb_scores = np.array(cb_scores)
        # Normalize scores to same scale
        cf_scores = (cf_scores - cf_scores.min()) / (cf_scores.max() - cf_scores.min())
        cb_scores = (cb_scores - cb_scores.min()) / (cb_scores.max() - cb_scores.min())
        hybrid_scores = self.alpha * cf_scores + (1 - self.alpha) * cb_scores
        # Create results DataFrame
        results = pd.DataFrame({
            'item_id': candidate_items['item_id'],
            'hybrid_score': hybrid_scores,
            'cf_score': cf_scores,
            'cb_score': cb_scores
        })
        return results.sort_values('hybrid_score', ascending=False).head(n)
    def save_model(self, path: str) -> None:
        """Save trained models to disk.
        Args:
            path: Directory path to save model files
        """
        model_path = Path(path)
        model_path.mkdir(parents=True, exist_ok=True)
        with open(model_path / 'cf_model.pkl', 'wb') as f:
            pickle.dump(self.cf_model, f)
        with open(model_path / 'tfidf.pkl', 'wb') as f:
            pickle.dump(self.tfidf, f)
        self.logger.info(f"Models saved to {path}")
    @classmethod
    def load_model(cls, interactions_path: str, metadata_path: str, 
                  model_path: str, alpha: float = 0.5) -> 'HybridRecommender':
        """Load trained models from disk.
        Args:
            interactions_path: Path to interactions CSV
            metadata_path: Path to item metadata CSV
            model_path: Directory containing saved models
            alpha: Hybrid weighting parameter
        Returns:
            Initialized HybridRecommender with loaded models
        """
        instance = cls(interactions_path, metadata_path, alpha)
        with open(Path(model_path) / 'cf_model.pkl', 'rb') as f:
            instance.cf_model = pickle.load(f)
        with open(Path(model_path) / 'tfidf.pkl', 'rb') as f:
            instance.tfidf = pickle.load(f)
        instance.logger.info(f"Models loaded from {model_path}")
        return instance
    def evaluate(self, test_size: float = 0.2) -> Dict[str, float]:
        """Evaluate model performance using train-test split.
        Args:
            test_size: Proportion of data to use for testing
        Returns:
            Dictionary with evaluation metrics (RMSE for CF, cosine similarity for CB)
        """
        # Collaborative filtering evaluation
        reader = Reader(rating_scale=(1, 5))
        data = Dataset.load_from_df(self.interactions[['user_id', 'item_id', 'rating']], reader)
        trainset, testset = train_test_split(data, test_size=test_size)
        self.cf_model.fit(trainset)
        predictions = self.cf_model.test(testset)
        cf_rmse = rmse(predictions)
        # Content-based evaluation (average similarity of recommended items to user's top items)
        all_users = self.interactions['user_id'].unique()
        cb_scores = []
        for user_id in all_users[:100]:  # Sample first 100 users for efficiency
            recommendations = self.recommend_items(user_id, n=5)
            top_items = self.interactions[
                self.interactions['user_id'] == user_id
            ].sort_values('rating', ascending=False)['item_id'].head(3).values
            for item_id in recommendations['item_id']:
                item_idx = self.item_data[self.item_data['item_id'] == item_id].index[0]
                for liked_item in top_items:
                    liked_idx = self.item_data[self.item_data['item_id'] == liked_item].index[0]
                    cb_scores.append(self.cosine_sim[item_idx][liked_idx])
        avg_cb_score = np.mean(cb_scores) if cb_scores else 0
        return {
            'cf_rmse': cf_rmse,
            'avg_cb_similarity': avg_cb_score
        }