import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
from surprise import Dataset, Reader, SVD
from surprise.model_selection import cross_validate, train_test_split
from surprise.accuracy import rmse
import warnings
from typing import Dict, List, Union, Optional, Tuple
import logging
from pathlib import Path
from dataclasses import dataclass
warnings.filterwarnings('ignore')
@dataclass
class RecommenderConfig:
    """Configuration for the hybrid recommender"""
    alpha: float = 0.5
    min_rating: float = 1.0
    max_rating: float = 5.0
    tfidf_max_features: int = 5000
    svd_n_factors: int = 100
class HybridRecommender:
    """Enhanced hybrid recommendation system combining collaborative and content-based filtering."""
    def __init__(self, interactions_path: str = 'user_item_interactions.csv',
                 metadata_path: str = 'item_metadata.csv', config: RecommenderConfig = None):
        """Initialize with configurable parameters."""
        self.config = config if config else RecommenderConfig()
        self._setup_logging()
        self.logger.info("Initializing HybridRecommender")
        try:
            interactions, item_data = self._load_data(interactions_path, metadata_path)
            self._validate_data(interactions, item_data)
            self._prepare_models(interactions, item_data)
        except Exception as e:
            self.logger.error(f"Initialization failed: {str(e)}")
            raise
    def _setup_logging(self) -> None:
        """Configure logging for the recommender."""
        self.logger = logging.getLogger('HybridRecommender')
        self.logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    def _load_data(self, interactions_path: str, metadata_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load and validate input data files."""
        interactions = pd.read_csv(interactions_path)
        item_data = pd.read_csv(metadata_path)
        return interactions, item_data
    def _validate_data(self, interactions: pd.DataFrame, item_data: pd.DataFrame) -> None:
        """Validate input dataframes have required columns and valid values."""
        required_interaction_cols = {'user_id', 'item_id', 'rating'}
        required_metadata_cols = {'item_id', 'description'}
        if not required_interaction_cols.issubset(interactions.columns):
            raise ValueError(f"Interactions data missing required columns: {required_interaction_cols - set(interactions.columns)}")
        if not required_metadata_cols.issubset(item_data.columns):
            raise ValueError(f"Item data missing required columns: {required_metadata_cols - set(item_data.columns)}")
        if interactions['rating'].min() < self.config.min_rating or interactions['rating'].max() > self.config.max_rating:
            raise ValueError(f"Ratings must be between {self.config.min_rating} and {self.config.max_rating}")
    def _prepare_models(self, interactions: pd.DataFrame, item_data: pd.DataFrame) -> None:
        """Prepare both collaborative and content-based models."""
        self._prepare_cf_model(interactions)
        self._prepare_cb_model(item_data)
        self.item_data = item_data
        self.interactions = interactions
    def _prepare_cf_model(self, interactions: pd.DataFrame) -> None:
        """Train collaborative filtering model."""
        reader = Reader(rating_scale=(self.config.min_rating, self.config.max_rating))
        data = Dataset.load_from_df(interactions[['user_id', 'item_id', 'rating']], reader)
        trainset = data.build_full_trainset()
        self.cf_model = SVD(n_factors=self.config.svd_n_factors)
        self.cf_model.fit(trainset)
        # Evaluate model
        cv_results = cross_validate(self.cf_model, data, measures=['RMSE'], cv=3, verbose=False)
        self.logger.info(f"CF Model trained - Avg RMSE: {np.mean(cv_results['test_rmse']):.4f}")
    def _prepare_cb_model(self, item_data: pd.DataFrame) -> None:
        """Prepare content-based recommendation components."""
        self.tfidf = TfidfVectorizer(stop_words='english', max_features=self.config.tfidf_max_features)
        tfidf_matrix = self.tfidf.fit_transform(item_data['description'])
        self.cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)
        self.item_indices = pd.Series(item_data.index, index=item_data['item_id']).drop_duplicates()
    def recommend_items(self, user_id: Union[int, str], item_id: Union[int, str] = None, n: int = 10) -> pd.DataFrame:
        """Generate hybrid recommendations for a user."""
        try:
            # Get collaborative filtering predictions
            cf_recs = self._get_cf_recommendations(user_id, n)
            # Get content-based recommendations if item_id provided
            cb_recs = self._get_cb_recommendations(item_id, n) if item_id else pd.DataFrame()
            # Combine recommendations
            return self._combine_recommendations(cf_recs, cb_recs, n)
        except Exception as e:
            self.logger.error(f"Recommendation failed for user {user_id}: {str(e)}")
            raise
    def _get_cf_recommendations(self, user_id: Union[int, str], n: int) -> pd.DataFrame:
        """Generate collaborative filtering recommendations."""
        all_items = self.interactions['item_id'].unique()
        predictions = [self.cf_model.predict(user_id, item_id) for item_id in all_items]
        predictions.sort(key=lambda x: x.est, reverse=True)
        top_n = predictions[:n]
        return pd.DataFrame({
            'item_id': [pred.iid for pred in top_n],
            'cf_score': [pred.est for pred in top_n],
            'type': ['cf'] * len(top_n)
        })
    def _get_cb_recommendations(self, item_id: Union[int, str], n: int) -> pd.DataFrame:
        """Generate content-based recommendations."""
        idx = self.item_indices[item_id]
        sim_scores = list(enumerate(self.cosine_sim[idx]))
        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
        sim_scores = sim_scores[1:n+1]  # Skip the item itself
        item_indices = [i[0] for i in sim_scores]
        return pd.DataFrame({
            'item_id': self.item_data['item_id'].iloc[item_indices],
            'cb_score': [i[1] for i in sim_scores],
            'type': ['cb'] * len(item_indices)
        })
    def _combine_recommendations(self, cf_recs: pd.DataFrame, cb_recs: pd.DataFrame, n: int) -> pd.DataFrame:
        """Combine recommendations using weighted hybrid approach."""
        if cb_recs.empty:
            return cf_recs.head(n)
        # Normalize scores
        cf_recs['cf_score'] = (cf_recs['cf_score'] - cf_recs['cf_score'].min()) / 
                             (cf_recs['cf_score'].max() - cf_recs['cf_score'].min())
        cb_recs['cb_score'] = (cb_recs['cb_score'] - cb_recs['cb_score'].min()) / 
                             (cb_recs['cb_score'].max() - cb_recs['cb_score'].min())
        # Merge recommendations
        merged = pd.merge(cf_recs, cb_recs, on='item_id', how='outer').fillna(0)
        merged['hybrid_score'] = (self.config.alpha * merged['cf_score'] + 
                                 (1 - self.config.alpha) * merged['cb_score'])
        return merged.sort_values('hybrid_score', ascending=False).head(n)